{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8510c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, Sampler\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import math\n",
    "\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"6\"\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d37cc27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cifar_dataset = load_dataset(\"uoft-cs/cifar10\")\n",
    "cifar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4bc7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnist_dataset = load_dataset(\"mnist\")\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d13335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "                         std=[0.2470, 0.2435, 0.2616])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "                         std=[0.2470, 0.2435, 0.2616])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d0260e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, vanila_dataset, image_indexes, transform):\n",
    "        self.dataset = vanila_dataset\n",
    "        self.image_indexes = image_indexes\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        selected_index = self.image_indexes[index]\n",
    "        \n",
    "        _image = self.dataset[selected_index]['img']\n",
    "        _image = self.transform(_image)\n",
    "\n",
    "        _label = self.dataset[selected_index]['label']\n",
    "\n",
    "        return _image, _label, selected_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c921e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    seed = worker_id + SEED\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc4a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_factory():\n",
    "    def __init__(self, train_dataset, test_dataset, sample_num, train_transform, valid_transform):\n",
    "        self.train_data_dict = {}\n",
    "\n",
    "        for i in range(0, train_dataset.num_rows):\n",
    "            _index = i\n",
    "            _label = train_dataset[i]['label']\n",
    "            if self.train_data_dict.get(_label) is None:\n",
    "                self.train_data_dict[_label] = []\n",
    "\n",
    "            self.train_data_dict[_label].append(_index)\n",
    "\n",
    "        self.train_datset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.selected_items = {_label: random.choices(item_index, k=min(len(item_index), sample_num)) for _label, item_index in self.train_data_dict.items()}\n",
    "\n",
    "        self.sample_num = sample_num\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "    def get_data_set(self):\n",
    "        _item_list =  sum(self.selected_items.values(), [])\n",
    "        random.shuffle(_item_list)\n",
    "        \n",
    "        self.train_set = custom_dataset(self.train_datset, _item_list, self.train_transform)\n",
    "        self.valid_set = custom_dataset(self.test_dataset, [i for i in range(0, 10000)], self.valid_transform)\n",
    "\n",
    "        print(f'Train set: {len(self.train_set.image_indexes)}')\n",
    "        print(f'Valid set: {len(self.valid_set.image_indexes)}')\n",
    "\n",
    "        return self.train_set, self.valid_set\n",
    "    \n",
    "    def get_data_loader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_set, pin_memory=True,\n",
    "            batch_size=32,\n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            self.valid_set, pin_memory=True,\n",
    "            batch_size=16,\n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def collect_losses(self, _paths, _labels, _losses):\n",
    "        for _path, _label, _loss in zip(_paths, _labels, _losses):\n",
    "            self.losses.append((_path.item(), _label.item(), _loss.item()))\n",
    "\n",
    "    def renew_data_loader(self, resample_ratio):\n",
    "        selected_items_dict = {}\n",
    "        for _path, _label, _loss in self.losses:\n",
    "            if selected_items_dict.get(_label) == None:\n",
    "                selected_items_dict[_label] = []\n",
    "\n",
    "            selected_items_dict[_label].append((_path, _loss))\n",
    "        \n",
    "        selected_items = {}\n",
    "        for _label, _path_list in selected_items_dict.items():\n",
    "            _paths, _weights = zip(*_path_list)\n",
    "            _weights = [(_weight - min(_weights))/(max(_weights)-min(_weights)) for _weight in _weights]\n",
    "\n",
    "            selected_items[_label] = random.choices(_paths,\n",
    "                                                    k=int(self.sample_num * (1-resample_ratio)),\n",
    "                                                    weights=_weights)\n",
    "        \n",
    "        unselected_items = {_label: random.sample([_path for _path in _path_list if _path not in selected_items_dict[_label]],\n",
    "                                                  k=int(self.sample_num * resample_ratio)) for _label, _path_list in self.train_data_dict.items()}\n",
    "\n",
    "        self.selected_items = {}\n",
    "        for (_label, _selected_path_list), (_label, _unselected_path_list) in zip(selected_items.items(), unselected_items.items()):\n",
    "            self.selected_items[_label] = []\n",
    "            self.selected_items[_label].extend(_selected_path_list)\n",
    "            self.selected_items[_label].extend(_unselected_path_list)\n",
    "\n",
    "        # print(self.selected_items)\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        self.get_data_set()\n",
    "\n",
    "        return self.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496a45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, selected_indexes in tqdm(valid_loader, desc=f'Validating', leave=False):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy().flatten())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_probs_roc = np.array(all_probs).reshape(-1, 10)\n",
    "    all_probs = np.array([np.argmax(_pred) for _pred in all_probs_roc])\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_probs),\n",
    "        'precision': precision_score(all_labels, all_probs, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_probs, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_probs, average='weighted', zero_division=0),\n",
    "        'roc_auc':roc_auc_score(all_labels, all_probs_roc, multi_class='ovr'),\n",
    "        # 'confusion_matrix': confusion_matrix(all_labels, all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d3719d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "object_dataset = 'cifar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 10000\n",
      "Valid set: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 10000\n",
      "Valid set: 10000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\workspace\\\\paper_1\\\\results/cifar/collect_losses_train_history_1000.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m json_path = os.path.join(os.getcwd(), \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/collect_losses_train_history_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_nums\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     55\u001b[39m os.makedirs(os.path.basename(json_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8-sig\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[32m     58\u001b[39m      json.dump(train_history, json_file, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\workspace\\\\paper_1\\\\results/cifar/collect_losses_train_history_1000.json'"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "\n",
    "base_model = timm.create_model('resnet50', pretrained=False, num_classes=10).to('cpu')\n",
    "\n",
    "for sample_nums in [1000, 1250, 2500]:\n",
    "    datafactory = custom_data_factory(\n",
    "        eval(f\"{object_dataset}_dataset['train']\"),\n",
    "        eval(f\"{object_dataset}_dataset['test']\"),\n",
    "        sample_num=sample_nums,\n",
    "        train_transform=train_transform,\n",
    "        valid_transform=valid_transform\n",
    "    )\n",
    "\n",
    "    model = timm.create_model('resnet50', pretrained=False, num_classes=10)\n",
    "    model.load_state_dict(base_model.state_dict())\n",
    "\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "    train_history = {}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    datafactory.get_data_set()\n",
    "    train_loader, valid_loader = datafactory.get_data_loader()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_losses = []\n",
    "\n",
    "        for images, labels, selected_index in tqdm(train_loader, desc=f'Training epoch: {epoch+1}/200', leave=False):\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_losses.append(loss.mean().item())\n",
    "            \n",
    "            datafactory.collect_losses(selected_index, labels.cpu(), [loss.cpu()])\n",
    "\n",
    "        valid_results = evaluate(model, valid_loader=valid_loader)\n",
    "        valid_results['loss'] = np.mean(total_losses)\n",
    "        train_history[epoch] = valid_results\n",
    "\n",
    "        train_loader, valid_loader = datafactory.renew_data_loader(resample_ratio=0.7)\n",
    "\n",
    "        json_path = os.path.join(os.getcwd(), rf'results/{object_dataset}/collect_losses_train_history_{sample_nums}.json')\n",
    "        os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "\n",
    "        with open(json_path, 'w', encoding='utf-8-sig') as json_file:\n",
    "             json.dump(train_history, json_file, indent=2, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f832cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m outputs = model(images)\n\u001b[32m     41\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m optimizer.step()\n\u001b[32m     46\u001b[39m total_losses.append(loss.mean().item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# model = timm.create_model('resnet50', pretrained=False, num_classes=10)\n",
    "model = torch.load('base_model.pt', weights_only=False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "train_history = {}\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_set = custom_dataset(mnist_dataset['train'], [i for i in range(0, 50000)], train_transform)\n",
    "valid_set = custom_dataset(mnist_dataset['test'], [i for i in range(0, 10000)], valid_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, pin_memory=True,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set, pin_memory=True,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_losses = []\n",
    "\n",
    "    for images, labels, selected_index in tqdm(train_loader, desc=f'Training epoch: {epoch+1}/200', leave=False):\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losses.append(loss.mean().item())\n",
    "\n",
    "    valid_results = evaluate(model, valid_loader=valid_loader)\n",
    "    valid_results['loss'] = np.mean(total_losses)\n",
    "    train_history[epoch] = valid_results\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(), f'results/mnist_base_train_history.json'), 'w', encoding='utf-8-sig') as json_file:\n",
    "        json.dump(train_history, json_file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d5459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a482b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
