{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8510c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, Sampler\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"6\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d37cc27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "cifar_dataset = load_dataset('uoft-cs/cifar10')\n",
    "cifar_dataset\n",
    "\n",
    "# mnist_dataset = load_dataset(\"mnist\", trust_remote_code=True) \n",
    "# mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb61045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_base_dir = r'C:\\datasets\\tiny-imagenet-200'\n",
    "\n",
    "# train_ds = load_dataset(\"imagefolder\", data_dir=os.path.join(imagenet_base_dir, 'train_preprocess'))[\"train\"]\n",
    "# test_ds  = load_dataset(\"imagefolder\", data_dir=os.path.join(imagenet_base_dir, 'valid_preprocess'))['validation']\n",
    "\n",
    "# imagenet_dataset = DatasetDict({\n",
    "#     \"train\": train_ds,\n",
    "#     \"test\": test_ds\n",
    "# })\n",
    "\n",
    "# imagenet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d13335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    # transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "    #                      std=[0.2470, 0.2435, 0.2616])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    # transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "    #                      std=[0.2470, 0.2435, 0.2616])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0260e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, vanila_dataset, image_indexes, transform):\n",
    "        self.dataset = vanila_dataset\n",
    "        self.image_indexes = image_indexes\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        selected_index = self.image_indexes[index]\n",
    "        \n",
    "        _image = self.dataset[selected_index]['img']\n",
    "        if isinstance(_image, str):\n",
    "            _image = Image.open(_image).convert('RGB')\n",
    "\n",
    "        _image = self.transform(_image)\n",
    "\n",
    "        _label = self.dataset[selected_index]['label']\n",
    "\n",
    "        return _image, _label, selected_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c921e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    seed = worker_id + SEED\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc4a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_factory():\n",
    "    def __init__(self, train_dataset, test_dataset, train_transform, valid_transform, sample_num:int=None, normalize_method:str=\"base\", batch_sizes:int=None, save_path:str=None):\n",
    "        self.train_data_dict = {}\n",
    "\n",
    "        for i in range(0, train_dataset.num_rows):\n",
    "            _index = i\n",
    "            _label = train_dataset[i]['label']\n",
    "            if self.train_data_dict.get(_label) is None:\n",
    "                self.train_data_dict[_label] = []\n",
    "\n",
    "            self.train_data_dict[_label].append(_index)\n",
    "\n",
    "        self.train_datset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.sample_num = sample_num\n",
    "        self.selected_items = {_label: random.choices(item_index, k=min(len(item_index), self.sample_num)) for _label, item_index in self.train_data_dict.items()}\n",
    "        \n",
    "        assert normalize_method in ['base', 'softmax'], 'Normalize method error, methods should be one of [\"base\", \"softmax\"]'\n",
    "        self.normalize_method = normalize_method\n",
    "        self.save_path = save_path if save_path else None\n",
    "        self.batch_sizes = batch_sizes if batch_sizes else 128\n",
    "\n",
    "        self.train_transform = train_transform\n",
    "        self.valid_transform = valid_transform\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "    def get_data_set(self):\n",
    "        _item_list =  sum(self.selected_items.values(), [])\n",
    "        random.shuffle(_item_list)\n",
    "        \n",
    "        self.train_set = custom_dataset(self.train_datset, _item_list, self.train_transform)\n",
    "        self.valid_set = custom_dataset(self.test_dataset, [i for i in range(0, 10000)], self.valid_transform)\n",
    "\n",
    "        # print(f'Train set: {len(self.train_set.image_indexes)}')\n",
    "        # print(f'Valid set: {len(self.valid_set.image_indexes)}')\n",
    "\n",
    "        return self.train_set, self.valid_set\n",
    "    \n",
    "    def get_data_loader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_set, pin_memory=True,\n",
    "            batch_size=self.batch_sizes,\n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            self.valid_set, pin_memory=True,\n",
    "            batch_size=self.batch_sizes,\n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # self.save_data_state()\n",
    "\n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def collect_losses(self, _paths, _labels, _losses):\n",
    "        assert len(_paths) == len(_labels), 'Losses information error'\n",
    "        assert len(_paths) == len(_losses), 'Losses information error'\n",
    "        \n",
    "        for _path, _label, _loss in zip(_paths, _labels, _losses):\n",
    "            self.losses.append((_path.item(), _label.item(), _loss.item()))\n",
    "\n",
    "    def save_data_state(self):\n",
    "        if not self.save_path:\n",
    "            return\n",
    "        \n",
    "        _save_path = os.path.join(os.getcwd(), self.save_path)\n",
    "        if os.path.exists(_save_path):\n",
    "            with open(_save_path, 'r', encoding='utf-8-sig') as json_file:\n",
    "                saved_states = json.load(json_file)\n",
    "            saved_states.append(self.selected_items)\n",
    "\n",
    "        else:\n",
    "            os.makedirs(os.path.dirname(_save_path), exist_ok=True)\n",
    "            saved_states = [self.selected_items]\n",
    "        \n",
    "        with open(_save_path, 'w', encoding='utf-8-sig') as json_file:\n",
    "            json.dump(saved_states, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def renew_data_loader(self, resample_ratio):\n",
    "        start_time = time.time()\n",
    "\n",
    "        selected_items_dict = {}\n",
    "        for _path, _label, _loss in self.losses:\n",
    "            if selected_items_dict.get(_label) == None:\n",
    "                selected_items_dict[_label] = []\n",
    "\n",
    "            selected_items_dict[_label].append((_path, _loss))\n",
    "        \n",
    "        selected_items = {}\n",
    "        for _label, _path_list in selected_items_dict.items():\n",
    "            _paths, _weights = zip(*_path_list)\n",
    "            if max(_weights) - min(_weights):\n",
    "                if self.normalize_method == 'base':\n",
    "                    _weights = [(_weight - min(_weights))/(max(_weights)-min(_weights) + 1e-8) for _weight in _weights]\n",
    "                \n",
    "                elif self.normalize_method == 'softmax':\n",
    "                    _weights = np.array(_weights)\n",
    "                    _weights = np.exp(_weights - np.max(_weights))\n",
    "                    _weights = _weights / np.sum(_weights)\n",
    "\n",
    "            else:\n",
    "               _weights = [1 / len(_weights)] * len(_weights)\n",
    "\n",
    "            selected_items[_label] = random.choices(_paths,\n",
    "                                                    k=int(self.sample_num * (1-resample_ratio)),\n",
    "                                                    weights=_weights)\n",
    "\n",
    "        unselected_items = {_label: random.sample([_path for _path in _path_list if _path not in selected_items_dict[_label]],\n",
    "                                                  k=min(int(self.sample_num * resample_ratio), len(self.train_data_dict[_label])))\n",
    "                                                  for _label, _path_list in self.train_data_dict.items()}\n",
    "\n",
    "        self.selected_items = {}\n",
    "        for (_label, _selected_path_list), (_label, _unselected_path_list) in zip(selected_items.items(), unselected_items.items()):\n",
    "            self.selected_items[_label] = []\n",
    "            self.selected_items[_label].extend(_selected_path_list)\n",
    "            self.selected_items[_label].extend(_unselected_path_list)\n",
    "            random.shuffle(self.selected_items[_label])\n",
    "\n",
    "        # print(self.selected_items)\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        self.get_data_set()\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        return self.get_data_loader(), end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d442f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_items_dict = {}\n",
    "# for _path, _label, _loss in datafactory.losses:\n",
    "#     if selected_items_dict.get(_label) == None:\n",
    "#         selected_items_dict[_label] = []\n",
    "\n",
    "#     selected_items_dict[_label].append((_path, _loss))\n",
    "\n",
    "# selected_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "496a45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, selected_indexes in tqdm(valid_loader, desc=f'Validating', leave=False):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())   \n",
    "    \n",
    "    all_probs_roc = np.concatenate(all_probs, axis=0)\n",
    "    all_preds = np.argmax(all_probs_roc, axis=1)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "        'roc_auc':roc_auc_score(all_labels, all_probs_roc, multi_class='ovr'),\n",
    "        # 'confusion_matrix': confusion_matrix(all_labels, all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3719d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import asdict\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class config:\n",
    "    epoch:int = 1000\n",
    "    lr:float = 0.01\n",
    "    \n",
    "    model_name:str = 'resnet50'\n",
    "    trained_model:bool = True\n",
    "\n",
    "    resample_ratio:float = 0.7\n",
    "    batch_size:int = 128\n",
    "    object_dataset:str = 'cifar'\n",
    "    normalize_method:str = 'base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bd9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = timm.create_model(config.model_name, pretrained=config.trained_model, num_classes=200).to('cpu')\n",
    "# torch.save(timm.create_model('resnet50', pretrained=True, num_classes=200), os.path.join(os.getcwd(), 'trained_base_model_200.pt'))\n",
    "# torch.save(timm.create_model('resnet50', pretrained=False, num_classes=200), os.path.join(os.getcwd(), 'untrained_base_model_200.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0000d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yaml_path = os.path.join(os.getcwd(), 'results', config.object_dataset, 'config.yaml')\n",
    "os.makedirs(os.path.dirname(yaml_path), exist_ok=True)\n",
    "\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml.dump(asdict(config()), yaml_file)\n",
    "\n",
    "\n",
    "# for sample_nums in [100, 125, 170, 250]:\n",
    "for sample_nums in [1000, 1500, 2000, 3000]:\n",
    "    datafactory = custom_data_factory(\n",
    "        eval(f\"{config.object_dataset}_dataset['train']\"),\n",
    "        eval(f\"{config.object_dataset}_dataset['test']\"),\n",
    "        sample_num=sample_nums,\n",
    "        train_transform=train_transform,\n",
    "        valid_transform=valid_transform,\n",
    "        batch_sizes=config.batch_size,\n",
    "        save_path=rf'results\\{config.object_dataset}\\dataset_history.json'\n",
    "    )\n",
    " \n",
    "    model = torch.load(os.path.join(os.getcwd(), 'trained_base_model.pt'),  weights_only=False) if config.trained_model else \\\n",
    "        torch.load(os.path.join(os.getcwd(), 'untrained_base_model.pt'),  weights_only=False)\n",
    "\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "    train_history = {}\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    datafactory.get_data_set()\n",
    "    train_loader, valid_loader = datafactory.get_data_loader()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(config.epoch):\n",
    "        model.train()\n",
    "        total_losses = []\n",
    "\n",
    "        for images, labels, selected_index in tqdm(train_loader, desc=f'Training epoch: {epoch+1:>3}/{config.epoch}', leave=False):\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_losses.append(loss.mean().item())\n",
    "            \n",
    "            datafactory.collect_losses(selected_index, labels.cpu(), loss.cpu())\n",
    "\n",
    "        valid_results = evaluate(model, valid_loader=valid_loader)\n",
    "        valid_results['loss'] = np.mean(total_losses)\n",
    "        valid_results['time'] = [time.time() - start_time]\n",
    "        train_history[epoch] = valid_results\n",
    "\n",
    "        (train_loader, valid_loader), spend_time = datafactory.renew_data_loader(resample_ratio=config.resample_ratio)\n",
    "        valid_results['time'].append(spend_time)\n",
    "\n",
    "        json_path = os.path.join(os.getcwd(), rf'results/{config.object_dataset}/collect_losses_train_history_{sample_nums}.json')\n",
    "        os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    " \n",
    "        with open(json_path, 'w', encoding='utf-8-sig') as json_file: \n",
    "            json.dump(train_history, json_file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f832cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "model = torch.load(os.path.join(os.getcwd(), 'trained_base_model.pt'),  weights_only=False) if config.trained_model else \\\n",
    "    torch.load(os.path.join(os.getcwd(), 'untrained_base_model.pt'),  weights_only=False)\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "train_history = {}\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "train_set = custom_dataset(eval(f\"{config.object_dataset}_dataset['train']\"), [i for i in range(0, 50000)], train_transform)\n",
    "valid_set = custom_dataset(eval(f\"{config.object_dataset}_dataset['test']\"), [i for i in range(0, 10000)], valid_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, pin_memory=True,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set, pin_memory=True,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(config.epoch):\n",
    "    model.train()\n",
    "    total_losses = []\n",
    "\n",
    "    for images, labels, selected_index in tqdm(train_loader, desc=f'Training epoch: {epoch+1:>3}/{config.epoch}', leave=False):\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losses.append(loss.mean().item())\n",
    "        \n",
    "    valid_results = evaluate(model, valid_loader=valid_loader)\n",
    "    valid_results['loss'] = np.mean(total_losses)\n",
    "    valid_results['time'] = time.time() - start_time\n",
    "    train_history[epoch] = valid_results\n",
    "\n",
    "    json_path = os.path.join(os.getcwd(), rf'results/{config.object_dataset}/base_history.json')\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8-sig') as json_file:\n",
    "        json.dump(train_history, json_file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d5459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a482b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
